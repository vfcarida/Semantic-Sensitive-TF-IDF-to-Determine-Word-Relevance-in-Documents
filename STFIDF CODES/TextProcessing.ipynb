{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#lines_per_file = 100\n",
    "#input_file = \"/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/output_Treatro.xml\"\n",
    "input_file = \"/home/amir/data/processed_12_TFIDF.xml\"\n",
    "output_file = \"/home/amir/data/processed_13_TFIDF.xml\"\n",
    "counter = 0\n",
    "smallfile = None\n",
    "processedFile = open(output_file,\"w\", encoding=\"utf-8\")\n",
    "counter = 0\n",
    "with open(input_file, encoding=\"utf-8\") as bigfile:\n",
    "    for line in bigfile:\n",
    "        line = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\",\" \",line)\n",
    "        processedFile.write(line)\n",
    "    processedFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Removing stopwords\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_file = \"/home/amir/data/processed_13_TFIDF.xml\"\n",
    "output_file = \"/home/amir/data/processed_14_TFIDF.xml\"\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.add(\"i'm\")\n",
    "stopWords.add(\"i’ll\")\n",
    "stopWords.add(\"i’d\")\n",
    "stopWords.add(\"he’ll\")\n",
    "\n",
    "stopWords.add(\"it’s\")\n",
    "stopWords.add(\"let’s\")\n",
    "stopWords.add(\"that’s\")\n",
    "stopWords.add(\"i’ve\")\n",
    "\n",
    "stopWords.add(\"i'm\")\n",
    "stopWords.add(\"i'll\")\n",
    "stopWords.add(\"i'd\")\n",
    "stopWords.add(\"he'll\")\n",
    "\n",
    "stopWords.add(\"it's\")\n",
    "stopWords.add(\"let's\")\n",
    "stopWords.add(\"that's\")\n",
    "stopWords.add(\"i've\")\n",
    "\n",
    "processedFile = open(output_file,\"w\", encoding=\"utf-8\")\n",
    "with open(input_file, encoding=\"utf-8\") as bigfile:\n",
    "    for line in bigfile:\n",
    "        line = ' '.join([word.lower() for word in line.split() if word.lower() not in stopWords])\n",
    "        if \"i'm\" in line.split():\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!\")\n",
    "            break\n",
    "        processedFile.write(line)\n",
    "        processedFile.write(\"\\n\")\n",
    "    processedFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Puts all the texts of a document in a single line\n",
    "input_file = \"/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/DevidedFiles/processed_11_TFIDF.xml\"\n",
    "output_file = \"/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/DevidedFiles/processed_12_TFIDF.xml\"\n",
    "docs = []\n",
    "counter = 0\n",
    "doc = \"\"\n",
    "with open (output_file, \"w\") as processedFile:\n",
    "    with open(input_file) as docs:\n",
    "        for line in docs:\n",
    "            line.replace(\"\\n\", \" \")\n",
    "            if '<item><separator>---------------------</separator></item>' in line:\n",
    "                if doc != None:\n",
    "                    doc = doc.replace(\"\\n\", \"\")\n",
    "                    processedFile.write(doc)\n",
    "                    processedFile.write(\"\\n\")\n",
    "                    doc = \"\"\n",
    "            else:\n",
    "                doc = doc + line\n",
    "                doc = doc + \" \"\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "# create a dataframe using texts and lables\n",
    "#trainDF = pandas.DataFrame()\n",
    "#trainDF['text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 'about', 'i’d', 'shan', 'mightn', 'isn', 'then', 'will', 'them', \"couldn't\", 'or', 'same', 'needn', 'nor', 'your', 'do', 'an', 'of', 'you', 'most', 'such', 'what', 'theirs', 'yourself', 'am', 'that', 'should', 'by', 'why', 'who', 'ma', 'aren', \"don't\", 'on', \"it's\", 'i’m', \"weren't\", 'for', 'if', 'hasn', 'here', 'had', 'with', 'those', 'which', 'no', 'didn', 'my', 'll', 'until', 'there', 'our', 'off', 'can', 'm', \"that'll\", 'd', 'they', 'just', 'shouldn', 'under', 'have', 'themselves', 'doing', 'some', 'yours', 'mustn', \"wasn't\", 'ain', 'y', 'that’s', 'him', 'after', 'during', 'did', 'but', 'while', 'down', 'above', \"didn't\", \"you've\", \"she's\", 'she', \"doesn't\", \"shouldn't\", 'wouldn', 'both', 'ourselves', 'up', 'i’ll', 'more', 'herself', 's', 'it’s', 'below', \"haven't\", 'i', 'than', 'through', \"wouldn't\", \"hadn't\", 'weren', 'how', 'having', 'in', 'myself', 'itself', 'o', 'are', 'between', 'any', 'couldn', 'and', 'before', 'haven', \"mightn't\", 'we', 'so', 'very', 'at', 've', 'ours', 'into', 'not', 'where', \"isn't\", 'it', 'from', 'his', 'again', 'other', \"you'd\", 'won', 'me', \"needn't\", 'yourselves', \"won't\", \"you're\", 'further', 'to', 're', 'is', 'only', \"shan't\", 'were', \"hasn't\", 'as', \"aren't\", 'hers', 'once', 'too', 'let’s', 'whom', 'himself', 't', 'their', 'against', 'the', 'each', 'a', 'when', 'all', 'doesn', 'because', \"mustn't\", 'her', 'hadn', 'he’ll', \"you'll\", 'few', 'does', 'being', 'has', 'was', 'be', 'he', 'been', 'this', 'over', 'its', \"should've\", 'now', 'wasn', 'out', 'don', 'own'}\n"
     ]
    }
   ],
   "source": [
    "#fline=open(\"/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/DevidedFiles/processed_12_TFIDF.xml\").readline().rstrip()\n",
    "#print(fline)\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.add(\"i'm\")\n",
    "stopWords.add(\"i’ll\")\n",
    "stopWords.add(\"i’d\")\n",
    "stopWords.add(\"he’ll\")\n",
    "\n",
    "stopWords.add(\"it’s\")\n",
    "stopWords.add(\"let’s\")\n",
    "stopWords.add(\"that’s\")\n",
    "stopWords.add(\"i’ve\")\n",
    "\n",
    "\n",
    "\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hd0/amir/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-78b3ebf020b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"i've\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#             cnt +=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_file = \"/home/amir/data/processed_14_TFIDF.xml\"\n",
    "cnt=0\n",
    "with open(input_file, encoding=\"utf-8\") as docs:\n",
    "        for line in docs:\n",
    "            if \"i've\" in line.split():\n",
    "                print(line)\n",
    "#             cnt +=1\n",
    "#             if cnt == 7:\n",
    "#                 print(line)\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Medications for Back Pain    Back Pain and Vitamins    Medications for Back Pain    Back Pain and Vitamins    Back Pain    Back Pain          I was at the stage of dementia where I couldn`t remember people names  I read on Gizmodo an article that dementia was caused by candida albicans in the brain  I started taking a borax solution of a rounded teaspoonful of borax dissolved in one liter of water  I took one teaspoonful (5mils) of a morning and at night  I came out of the darkness on the morning of the 6th day and have been taking it now for two years and I`ll take it for the rest of my life as the thought of going back into dementia is too frightening, You can look up the Alzheimers web page and confirm the candida albicans association  Read  the borax conspiracy , the dosage is there also borax takes calcium from arteries and places it back into bone where it belongs    This is MY father's Aricept experience - paranoia and aggression  Hard to convince doctors to stop the drug  It took 6 weeks for him to be himself again  Company has now removed paranoia and aggression as listed side effects BUT they continue to be a problem that is NOT AZ! When he realized he had been so hard on his family, he had a heart attack!   i stopped taking Prilosec after I heard it can cause dementia, Alzheimer's, take 25 yrs off your life as bad as smoking but now have withdrawal symptoms : chills, sweating, dizziness,, very bad heartburn, feeling sick, weak  Took over the counter Zantact and had flu like symptoms chills, swollen glans, feel like getting cold symptom's but don't  Am suffering bad    My husband has dementia and is increasingly having mobility problems  He is ataxic and has a hard time getting up from being seated  Is this mobility problem characteristic of Alzheimer's?   I was kidnapped by a scopolamine gang in Mexico City  Now I effectively suffer from PTSD, Alzheimers, and Mild Tourettes Syndrome  I did not before  I am 37 years old and this happened 2 years ago  I live in the US and cannot get help because the doctors won't entertain the idea that I know something they don't  It ruined every aspect of my life  Be careful (or don't) hang out with strangers in danger zones  I would have gotten away if I had known it existed  They told me it was coke and I did a huge rail before they dumped two grams in a glass of water and drenched me  Cuidate!   I use lavender oil  I mix Lavender oil with another oil like coconut, or olive, and I gently rub it on her temples and in between her eyebrows  She sleeps almost all night and only gets up around 5am to go use the washroom  I will also put on the diffuser with lavender oil in the evenings    I've read studies that it blocks minerals by making digestion less effective, so heart issues and Alzheimer's are problems, I don't Want that     I have had a duodenal ulcer since 2009, diagnosed in 2011, (pepcid helped before, hospitalized in 2011, and 2014 , last time it was 3 3 cm had to have 3 clamps to close it, was put on prilosec then, was told I needed it for 2 weeks, then could stop  It has been almost 2 years now, when i try to stop, vomiting returns    Better than without it  Was hoping would not need it on a daily basis, only for a while  Worry about it causing forgetfulness or worse - Alzheimer     Read a phrase: Movical lets you forget about constipation - and everything else  Very funny, but also disturbing    Had a stress ulcer starting in 2009, hospitalized 2011 and 2014 for blood loss, had clamps placed during scope, was told to take prilosec 2 weeks, couldn't get off  If I try , vomiting and stomach pain  However I don't want to end up with Alzheimer's and osteoporosis and heart failure    Thank you so much for sharing your experience  I'm also highly sensitive to various meds/supplements, and have been noticing a lot of changes (not the positive kind) since being on Buspar for just under 2 months  One of the biggest is a major increase in joint pain/stiffness especially in my knees (where my arthritis and joint damage is the worst)  I thought maybe it was our bi-polar Minnesota weather  But, today's a fairly nice day and the symptoms are just as bad  Another (scarier) issue is I feel I've been exhibiting bizarre behavior doing or feeling things I've never experienced before being more impulsive, easily irritated/angered, extra anxious or unsure of myself in social situations (even when among people I know well)  I don't like feeling unstable or not in control of my mind/body  My doctor told me that it takes about 2 months to achieve full benefit of this medication  So far, I haven't really found a lot of benefit just a lot of awful side effects :( Low doses of Clonazepam helped with multiple health issues for over a decade  But, ever since reports came out about an increased risk of having early onset dementia/Alzheimer's, many doctors will no longer prescribe Clonazepam (Klonopin), or other Benzodiazepines for their patients       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(\"/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/DevidedFiles/processed_12_TFIDF.xml\", encoding=\"utf-8\") as bigfile:\n",
    "    for line in bigfile:\n",
    "        counter +=1\n",
    "        if counter == 6:\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TF\n",
    "from collections import Counter\n",
    "allDocs = list()\n",
    "totalDocNum = 0\n",
    "def main():\n",
    "    output_file = \"/home/amir/data/docsWordProbs4.txt\"\n",
    "    #if not os.path.exists (afile):\n",
    "        \n",
    "    processedFile = open(output_file,\"w+\" , encoding=\"utf-8\")\n",
    "    #use open() for opening file.\n",
    "    #Always use `with` statement as it'll automatically close the file for you.\n",
    "    with open('/home/amir/data/processed_14_TFIDF.xml' , encoding=\"utf-8\") as f:\n",
    "        #create a list of all words fetched from the file using a list comprehension\n",
    "        #words = [word for line in f for word in line.split()]\n",
    "        for line in f:\n",
    "            #global totalDocNum\n",
    "            #totalDocNum = totalDocNum + 1\n",
    "            docWordsCount = list()\n",
    "            docWords = line.split()\n",
    "            totalWords = len(docWords)\n",
    "            c = Counter(docWords)\n",
    "            for word, count in c.most_common():\n",
    "                #docWordsCount.append((word,float(count)/totalWords))\n",
    "                processedFile.write(word + \":\" + str(float(count)/totalWords) + \" \\n\")\n",
    "            #allDocs.append(docWordsCount)\n",
    "            processedFile.write(\"############ \\n\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-82edf10533f5>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-82edf10533f5>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    uniqueWords[key] = 1uniqueWords\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#################Getting unique words\n",
    "\n",
    "#output_file = \"/home/amir/data/UniqueWords.txt\"\n",
    "\n",
    "counter = 0\n",
    "docsNum = 50000\n",
    "uniqueWords = {}\n",
    "with open('/home/amir/data/processed_14_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        counter +=1\n",
    "        words = line.split()\n",
    "#         if counter == 50:\n",
    "#             print(line)\n",
    "#             break\n",
    "        for word in words:\n",
    "            #processedFile = open(output_file,\"r+\")\n",
    "            #if word.lower() not in processedFile.read():\n",
    "            #processedFile.write(word.lower() + \" \")\n",
    "            key = word.lower()\n",
    "            uniqueWords[key] = uniqueWords\n",
    "#         if counter > docsNum:\n",
    "#             break\n",
    "                #uniqueWords.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from multiprocessing.dummy import Pool as ThreadPool \n",
    "# import time\n",
    "# list1= []\n",
    "# urls = [2,3,1,4,5,7,0,14,2,3,1,4,5,7,0,142,3,1,4,5,7,0,14]\n",
    "\n",
    "# def get_status(url):\n",
    "#     #list1.append(url)\n",
    "#     print(url)\n",
    "#     time.sleep(1)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     pool = ThreadPool(1) # Make the Pool of workers\n",
    "#     results = pool.map(get_status, urls) #Open the urls in their own threads\n",
    "#     pool.close() #close the pool and wait for the work to finish \n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Number of times the term has been repeated in the docs\n",
    "\n",
    "# from multiprocessing import Pool as ThreadPool\n",
    "# import threading\n",
    "# from collections import Counter\n",
    "# import multiprocessing\n",
    "# import resource\n",
    "# #resource.setrlimit(resource.RLIMIT_NOFILE, (1024,-1))\n",
    "# import time\n",
    "\n",
    "# my_lock = threading.Lock()\n",
    "\n",
    "# overallWordFrequencyDict = {}\n",
    "# docCounter = 0\n",
    "# def countWordDocFrequency(word):\n",
    "#     global someList\n",
    "#     global docsNum\n",
    "#     with my_lock:\n",
    "#         tdfCounter = 0\n",
    "#         counter2 = 0\n",
    "#         #print(\"Inspecting word: \", word)\n",
    "#         with open('/home/amir/data/processed_12_TFIDF.xml', encoding=\"utf-8\") as docs:\n",
    "#             for line in docs:\n",
    "#                 counter2 +=1\n",
    "#                 if word.lower() in line.split():\n",
    "#                     tdfCounter +=1\n",
    "#                 if counter2 > docsNum:\n",
    "#                     break\n",
    "#             #print(word,\" finished\")\n",
    "#             return (word,tdfCounter)\n",
    "    \n",
    "\n",
    "        \n",
    "# def test(val):\n",
    "#     print(val)\n",
    "#     time.sleep(1)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     termDocFrequency = list()\n",
    "#     pool = ThreadPool(24) # Make the Pool of workers\n",
    "#     #pool.map(countWordDocFrequency, (uniqueWords, docs.split(\"\\n\"))) #Open the words in their own threads\n",
    "#     #pool.map(test,[1,4,6,7,3,2,15,7,8,4,2,1,11,10])\n",
    "#     #pool.map(countWordDocFrequency, (uniqueWords))\n",
    "#     #result = pool.map(countWordDocFrequency, [\"I\",\"have\",\"medication\",\"headache\", \"knee\", \"knee\"])\n",
    "#     result = pool.map(countWordDocFrequency, list(uniqueWords.keys())[0:len(uniqueWords)])\n",
    "#     pool.close() #close the pool and wait for the work to finish \n",
    "#     pool.join()\n",
    "#     #print(someList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0,len(result)):\n",
    "#     overallWordFrequencyDict [result[i][0]] = result[i][1]\n",
    "# #overallWordFrequencyDict\n",
    "# #type(overallWordFrequencyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(overallWordFrequencyDict)\n",
    "# overallWordFrequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('/home/amir/data/DocsContainTerm2.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for key, value in uniqueWords.items():\n",
    "#         f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ###IDF\n",
    "# # Log (N/DocsContainTerm)\n",
    "# import math\n",
    "# idfDict = {}\n",
    "# for key,value in overallWordFrequencyDict.items():\n",
    "#     if value!=0 and '!' not in key:\n",
    "#         IDF = math.log((docsNum/value),10)\n",
    "#         idfDict[key] = IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##IDF João\n",
    "\n",
    "with open('/home/amir/data/processed_14_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        for term in set(line.split()):\n",
    "            if term not in uniqueWords:\n",
    "                uniqueWords[term] = 1\n",
    "            else:\n",
    "                uniqueWords[term] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberOfDocs = 0\n",
    "\n",
    "with open('/home/amir/data/processed_14_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        numberOfDocs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniqueWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d8a3f7918ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'uniqueWords' is not defined"
     ]
    }
   ],
   "source": [
    "len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "import operator\n",
    "import math\n",
    "\n",
    "with open('/home/amir/data/TF-IDF.txt', 'w', encoding=\"utf-8\") as tfidfScoresArquive:\n",
    "    with open('/home/amir/data/docsWordProbs4.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        dicts = {}\n",
    "        for line in f:\n",
    "            if '############' not in line:\n",
    "                word = line.split(':')[0].lower()\n",
    "                TF = line.split(':')[1]\n",
    "                IDF = math.log10(numberOfDocs/float(uniqueWords[word]))\n",
    "                TFIDF_score= (float(TF) * IDF)\n",
    "                dicts[word] = TFIDF_score\n",
    "            else:\n",
    "                sortedScores = sorted(dicts.items(),key=operator.itemgetter(1),reverse=True)\n",
    "                for item in sortedScores:\n",
    "                    tfidfScoresArquive.write(str(item))\n",
    "                    tfidfScoresArquive.write(\"\\n\")\n",
    "                tfidfScoresArquive.write('############')\n",
    "                tfidfScoresArquive.write('\\n')\n",
    "                dicts = {}\n",
    "                \n",
    "                #sortedScores = []\n",
    "#             else:\n",
    "#                 sortedScores = sorted(dic.items(),key=operator.itemgetter(1),reverse=True)\n",
    "#                 print(sortedScores)\n",
    "#                 for key, value in sortedScores.items():\n",
    "#                     tfidfScoresArquive.write('%s:%s\\n' % (key, value))\n",
    "#                 tfidfScoresArquive.write(\"##########\")\n",
    "#                 tfidfScoresArquive.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loading Google W2vec embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = '/home/amir/data/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsPerDoc = {}\n",
    "docConter = 0\n",
    "with open('/home/amir/data/processed_14_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        docCounter +=1\n",
    "        wordsPerDoc[docCounter] = len(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordsPerDoc\n",
    "docsMeanEmbeddings = {}\n",
    "def returnTopNMeanEmedding():\n",
    "    matrix = []\n",
    "    docNumber = 1\n",
    "    with open('/home/amir/data/TF-IDF.txt', 'r', encoding=\"utf-8\") as tfidfScoresArquive:\n",
    "        for line in tfidfScoresArquive:\n",
    "            print(\"!!!\")\n",
    "            if '##########' in line.split():\n",
    "                docNumber +=1\n",
    "                line = next(tfidfScoresArquive)\n",
    "            topRanks = round(math.sqrt(wordsPerDoc[docNumber]))\n",
    "            for i in range(0,topRanks):\n",
    "                print(line)\n",
    "                print(\"=======\")\n",
    "                word = line.split(',')[0].split('(')[1].split(\"'\")[1]\n",
    "                matrix.append(model[word])\n",
    "                line = next(tfidfScoresArquive)\n",
    "            #meanEmbedding = matrix.mean(axis=0)\n",
    "            while '##########' not in line.split():\n",
    "                line = next(tfidfScoresArquive)\n",
    "                break\n",
    "\n",
    "returnTopNMeanEmedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(wordsPerDoc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49776962600795116\n"
     ]
    }
   ],
   "source": [
    "matrix = []\n",
    "docNumber = 0\n",
    "with open('/home/amir/data/TF-IDF.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word = line.split(',')[1].split(')')[0]\n",
    "        print(word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dooxycline\n",
      "izonazid\n",
      "qpply\n",
      "roigh\n",
      "lidex\n",
      "oratance\n",
      "exsema\n",
      "cloben\n",
      "i’m\n"
     ]
    }
   ],
   "source": [
    "###Testing proposed TFIDF\n",
    "\n",
    "wordCounter = 0\n",
    "docNumber = 1\n",
    "matrix = []\n",
    "topRanks = 0\n",
    "with open('/home/amir/data/Test.txt', encoding=\"utf-8\") as tfidfScoresArquive:\n",
    "    global wordCounter\n",
    "    for line in tfidfScoresArquive:\n",
    "        wordCounter+=1\n",
    "        \n",
    "topRanks = round(math.sqrt(wordCounter))\n",
    "### create mean embedding\n",
    "counter = 0\n",
    "with open('/home/amir/data/Test.txt', encoding=\"utf-8\") as ff:\n",
    "    for line in ff:\n",
    "        counter +=1\n",
    "        if counter <=topRanks:\n",
    "            try:\n",
    "                topWordsConter = 0\n",
    "                word = line.split(',')[0].split('(')[1].split(\"'\")[1]\n",
    "                wordTFIDFScore = line.split(',')[1].split(')')[0]\n",
    "                embedding = model[word.title()]\n",
    "                matrix.append(embedding)\n",
    "            except:\n",
    "                print(line.split(',')[0].split('(')[1].split(\"'\")[1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "        ## return to the top of document\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training word2vec part 1\n",
    "sentenses = []\n",
    "with open('/home/amir/data/processed_13_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        sentenses.append(line.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training word2vec part2 \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentenses, min_count=1)\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(senteces,'/home/amir/data/medicationTxtModel.bin', binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model[\"oratance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701794"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.save_word2vec_format('gensimMedicationTxtModel.bin')\n",
    "\n",
    "#len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.similarity('oratance', 'sound')\n",
    "#sentenses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hd0/amir/Uni_Stuff/Research/DrugsInfo/DrugsInfo/spiders/DevidedFiles/processed_13_TFIDF.xml', 'r+', encoding=\"utf-8\") as docs:\n",
    "    for line in docs:\n",
    "        if \"dooxycline\" in line:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_exp = (x ** 2 for x in range(10) if x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen_exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
